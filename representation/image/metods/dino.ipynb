{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = \"../../..\"\n",
    "if path not in sys.path:\n",
    "    sys.path.insert(0, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data_retrieval import lipade_groundtruth\n",
    "from data_retrieval.tools.data_loader import getDataLoader\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights, resnet50, ResNet50_Weights\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizerFunc = optim.Adam\n",
    "temperature = 0.5\n",
    "learningRate = 1e-3\n",
    "batch_size = 512\n",
    "workers = 2\n",
    "corpus = \"lipade_groundtruth\"\n",
    "resultsPath = \"../results/distance/\" + corpus + \"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_,y = lipade_groundtruth.getDataset(mode = 'unique', uniform=True)\n",
    "\n",
    "images = []\n",
    "for i in range(len(x)):\n",
    "    images.append(Image.open(x[i]).convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5855, 256, 256, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "images = np.array(images)\n",
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = getDataLoader(images, None, None, False, batch_size, True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from degradations.methods.halftoning.floyd_steinberg import transforms_floyd_steinberg_halftoning\n",
    "from degradations.methods.halftoning.atkinson import transforms_atkinson_dithering\n",
    "from degradations.methods.halftoning.bayers_threshold import transforms_bayer_halftoning\n",
    "from degradations.methods.halftoning.dot_traditional import transforms_dot_halftoning  # Import your halftoning methods\n",
    "from degradations.methods.noise.gaussian_noise import transforms_add_gaussian_noise\n",
    "from degradations.methods.noise.salt_and_pepper import transforms_add_salt_and_pepper_noise\n",
    "from degradations.methods.noise.dirty_rollers import transforms_dirty_rollers\n",
    "#from degradations.methods.noise.film_grain import transforms_apply_film_grain # Import your noise methods\n",
    "from degradations.methods.paper.ink_bleed import transforms_ink_bleed  \n",
    "from degradations.methods.paper.crumpled_paper import transforms_crumpled_paper\n",
    "from degradations.methods.paper.folded_paper import transforms_folded_paper\n",
    "from degradations.methods.paper.bleedthrough import transforms_bleedthrough\n",
    "from degradations.methods.paper.scribbles import transforms_scribbles\n",
    "from degradations.methods.paper.stains import transforms_stains # Import your paper feel methods\n",
    "from degradations.methods.human_corrections.erased_element import transforms_erased_element # Import your human correction methods\n",
    "from degradations.methods.layout.picture_overlay import transforms_picture_overlay\n",
    "from degradations.methods.layout.text_overlay import transforms_text_overlay # Import your layout methods\n",
    "\n",
    "\n",
    "class transforms_SepiaFilter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(transforms_SepiaFilter, self).__init__()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sepia_filter = torch.tensor([[0.393, 0.769, 0.189],\n",
    "                                     [0.349, 0.686, 0.168],\n",
    "                                     [0.272, 0.534, 0.131]], device=batch.device)\n",
    "        batch = torch.einsum('ijkl,mj->imkl', batch, sepia_filter)\n",
    "        return batch.clamp(0, 1)\n",
    "\n",
    "\n",
    "class transforms_Rotate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(transforms_Rotate, self).__init__()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = batch.movedim(2,3)    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_degrad = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomResizedCrop(size=images.shape[2], scale=(2/3, 1), ratio=(1, 1))\n",
    "    ], p=1/3),\n",
    "\n",
    "    # Sepia\n",
    "    transforms.RandomApply([transforms_SepiaFilter()],p=1/3),\n",
    "\n",
    "    # halftone\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomChoice([\n",
    "            transforms_floyd_steinberg_halftoning(128),\n",
    "            transforms_atkinson_dithering(128)\n",
    "            ])\n",
    "        ], p=0.05),\n",
    "\n",
    "    # layout\n",
    "    transforms.RandomApply([\n",
    "            transforms.RandomChoice([\n",
    "                transforms_picture_overlay(),\n",
    "                transforms_text_overlay()\n",
    "            ])\n",
    "    ], p=0.2),\n",
    "\n",
    "    # erased\n",
    "    transforms.RandomApply([\n",
    "        transforms_erased_element()\n",
    "    ], p=0.1),\n",
    "\n",
    "    # noise\n",
    "    transforms.RandomApply([\n",
    "            transforms.RandomChoice([\n",
    "                transforms_add_gaussian_noise(),\n",
    "                transforms_add_salt_and_pepper_noise(),\n",
    "                transforms_dirty_rollers((8,10))\n",
    "            ])\n",
    "    ], p=0.1),\n",
    "\n",
    "    # stains\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomChoice([\n",
    "            #transforms_scribbles(),\n",
    "            transforms_stains(), \n",
    "            transforms_ink_bleed(),\n",
    "            transforms_bleedthrough(),\n",
    "        ])\n",
    "    ], p=0.3),\n",
    "\n",
    "    # texture\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomChoice([\n",
    "            transforms_crumpled_paper(),\n",
    "            transforms_folded_paper(0.4),   \n",
    "        ])\n",
    "    ], p=0.2),\n",
    "\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2), \n",
    "    transforms.GaussianBlur(kernel_size=9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO_Representation(nn.Module):\n",
    "    def __init__(self, encoder, in_dim=2048, out_dim=128, m=0.99):\n",
    "        super(DINO_Representation, self).__init__()\n",
    "\n",
    "        # Student encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Projection head for student\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "        # Momentum encoder (teacher)\n",
    "        self.momentum_encoder = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.momentum_encoder.fc = nn.Identity()\n",
    "\n",
    "        # Set momentum encoder to eval mode and frozen\n",
    "        self.momentum_encoder.eval()\n",
    "        for param in self.momentum_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Momentum update coefficient\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, x, x_2):\n",
    "        student_repr = self.encoder(x_2)\n",
    "        student_z = self.projection_head(student_repr)\n",
    "        \n",
    "        # Apply the momentum update rule to the teacher\n",
    "        with torch.no_grad():\n",
    "            # Momentum update of the teacher encoder\n",
    "            for param_q, param_k in zip(self.encoder.parameters(), self.momentum_encoder.parameters()):\n",
    "                param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "        \n",
    "        teacher_repr = self.momentum_encoder(x)\n",
    "        teacher_z = self.projection_head(teacher_repr)\n",
    "        \n",
    "        return student_z, teacher_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "representationEncoder = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "representationEncoder.fc = nn.Identity()\n",
    "\n",
    "model = DINO_Representation(representationEncoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(DINOLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, student_z, teacher_z):\n",
    "        # Normalize the projections to unit length\n",
    "        student_z = F.normalize(student_z, dim=-1, p=2)\n",
    "        teacher_z = F.normalize(teacher_z, dim=-1, p=2)\n",
    "\n",
    "        # Compute the DINO loss (cross-entropy loss between student and teacher)\n",
    "        loss = -(student_z * teacher_z.detach()).sum(dim=-1).mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DINOLoss(temperature=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs/DINO\")\n",
    "\n",
    "\n",
    "losses_all = []\n",
    "for epoch in range(epochs):  \n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for x in tqdm(trainLoader, desc=\"Epoch \" + str(epoch)):\n",
    "        x = x.to(device)\n",
    "        x_2 = transform_degrad(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        student_z, teacher_z = model(x,x_2)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(student_z, teacher_z)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", torch.tensor(losses).mean(), epoch)\n",
    "    losses_all.append(torch.tensor(losses).mean())\n",
    "    torch.save(model.state_dict(), \"model_dino.pth\")\n",
    "\n",
    "plt.plot(losses_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
